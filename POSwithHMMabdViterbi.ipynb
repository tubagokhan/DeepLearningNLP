{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMnj8FAQbAKpCwJArBzj5kb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/DeepLearningNLPFoundations/blob/main/POSwithHMMabdViterbi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import pprint\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('treebank')"
      ],
      "metadata": {
        "id": "e7CtHfMyoocn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyY3V4OVRuD7"
      },
      "outputs": [],
      "source": [
        "# reading the Treebank tagged sentences\n",
        "train_set= list(nltk.corpus.treebank.tagged_sents())\n",
        "\n",
        "# Getting list of tagged words\n",
        "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
        "\n",
        "# tokens \n",
        "tokens = [pair[0] for pair in train_tagged_words]\n",
        "# vocabulary\n",
        "V = set(tokens)\n",
        "print(\"Total vocabularies: \",len(V))\n",
        "# number of tags\n",
        "T = set([pair[1] for pair in train_tagged_words])\n",
        "print(\"Total tags: \",len(T))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Emission Probability"
      ],
      "metadata": {
        "id": "Qwkig-VhSCrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing P(w/t) and storing in T x V matrix\n",
        "t = len(T)\n",
        "v = len(V)\n",
        "w_given_t = np.zeros((t, v))\n",
        "\n",
        "# compute word given tag: Emission Probability\n",
        "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
        "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
        "    count_tag = len(tag_list)\n",
        "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
        "    count_w_given_tag = len(w_given_tag_list)\n",
        "    \n",
        "    return (count_w_given_tag, count_tag)\n"
      ],
      "metadata": {
        "id": "JE6RPOvXR-_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transition Probability"
      ],
      "metadata": {
        "id": "R6__yvd5SMuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
        "\n",
        "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
        "    tags = [pair[1] for pair in train_bag]\n",
        "    count_t1 = len([t for t in tags if t==t1])\n",
        "    count_t2_t1 = 0\n",
        "    for index in range(len(tags)-1):\n",
        "        if tags[index]==t1 and tags[index+1] == t2:\n",
        "            count_t2_t1 += 1\n",
        "    return (count_t2_t1, count_t1)\n",
        "\n",
        "# creating t x t transition matrix of tags\n",
        "# each column is t2, each row is t1\n",
        "# thus M(i, j) represents P(tj given ti)\n",
        "\n",
        "tags_matrix = np.zeros((len(T), len(T)), dtype='float32') # transition matrix\n",
        "for i, t1 in enumerate(list(T)):\n",
        "    for j, t2 in enumerate(list(T)): \n",
        "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
        "\n",
        "# convert the matrix to a df for better readability\n",
        "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))"
      ],
      "metadata": {
        "id": "qFV2j9OASIvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Viterbi "
      ],
      "metadata": {
        "id": "dTQOjMbzScc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Viterbi Heuristic\n",
        "def Viterbi(words, train_bag = train_tagged_words):\n",
        "    state = []\n",
        "    T = list(set([pair[1] for pair in train_bag]))\n",
        "    \n",
        "    for key, word in enumerate(words):\n",
        "        #initialise list of probability column for a given observation\n",
        "        p = [] \n",
        "        for tag in T:\n",
        "            if key == 0:\n",
        "                transition_p = tags_df.loc['.', tag]\n",
        "            else:\n",
        "                transition_p = tags_df.loc[state[-1], tag]\n",
        "                \n",
        "            # compute emission and state probabilities\n",
        "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
        "            state_probability = emission_p * transition_p    \n",
        "            p.append(state_probability)\n",
        "            \n",
        "        pmax = max(p)\n",
        "        # getting state for which probability is maximum\n",
        "        state_max = T[p.index(pmax)] \n",
        "        state.append(state_max)\n",
        "    return list(zip(words, state))"
      ],
      "metadata": {
        "id": "nbO7F0AlSX1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing\n",
        "sentence_test = 'Twitter is the best networking social site. Man is a social animal. Data science is an emerging field. Data science jobs are high in demand.'\n",
        "words = word_tokenize(sentence_test)\n",
        "tagged_seq = Viterbi(words)\n",
        "print(tagged_seq)"
      ],
      "metadata": {
        "id": "hzMmXBFxShNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing\n",
        "sentence_test = 'Tuba have an interview  for a postdoc position. '\n",
        "words = word_tokenize(sentence_test)\n",
        "tagged_seq = Viterbi(words)\n",
        "print(tagged_seq)"
      ],
      "metadata": {
        "id": "MtIvoBzaq8Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newWord=('Tuba','NNP')\n",
        "\n",
        "if newWord in train_tagged_words:\n",
        "\tprint (\"Yes\")\n",
        "else:\n",
        "  train_tagged_words.append(newWord)\n",
        "  print(\"Added\")"
      ],
      "metadata": {
        "id": "ADrWZMxhtJfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing\n",
        "sentence_test = 'Tuba have an interview  for a postdoc position.'\n",
        "words = word_tokenize(sentence_test)\n",
        "tagged_seq = Viterbi(words)\n",
        "print(tagged_seq)"
      ],
      "metadata": {
        "id": "iSNeWptDs0O-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}